#To start JDBC Connection

# pyspark --packages org.postgresql:postgresql:9.4.1211 --driver-class-path /home/hadoop/postgresql-9.4.1211.jar --jars /home/hadoop/postgresql-9.4.1211.jar

from pyspark import SparkContext
from pyspark.sql import SparkSession
from datetime import date, time

sc = SparkContext()
spark = SparkSession(sc)

# vars for tables and dataFrames
data_frames = {}
tables = ["customer", "orders", "lineitem", "supplier", "nation", "region", "part", "partsupp"]

# DB credentials
user ="user"
password="pass"

# DB connection parameters
jdbc_url="jdbc:postgresql://user.cgjfkjdlsfjsg.us-region-1.rds.amazonaws.com:5432/postgres"


def return_dataframe():
    for table in tables:
        # create df
        data_frames["df_" + table] = spark.read \
            .jdbc(jdbc_url, table,
                  properties={"user": user, "password": password})
        # register tempTables
        data_frames["df_" + table].registerTempTable(table)

# calling our udf to load tables
return_dataframe()

# init vars
runtimes = []

# Function for running our queries
def run_benchmark_query(query, message):
    print("Starting: " + message)
    
    # start time
    query_start_time = datetime.now()
    
    # run the query and show the result
    df = spark.sql(query)
    df.show(100)
    
    # end time
    query_stop_time = datetime.now()
    
    run_time = (query_stop_time-query_start_time).seconds
    print("Runtime: %s seconds" % run_time)
    runtimes.append(run_time)
    print("Finishing: " + message)

benchmarks = []

# ========================
run_benchmark_query("""
SELECT
    n_name,
    avg(o_totalprice)
FROM
    customer,
    orders,
    lineitem,
    supplier,
    nation,
    region
WHERE
    c_custkey = o_custkey
    AND l_orderkey = o_orderkey
    AND l_suppkey = s_suppkey
    AND c_nationkey = s_nationkey
    AND s_nationkey = n_nationkey
    AND n_regionkey = r_regionkey
    AND o_orderpriority = '5-LOW'
GROUP BY
    n_name
""", "Run 11")
